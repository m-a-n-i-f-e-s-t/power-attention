# Power Attention

A CUDA implementation of symmetric power attention, achieving transformer-level performance with linear-cost RNN computation.

## Getting Started

- [Installation](installation.md): Build configuration and requirements
- [Quickstart](quickstart.md): API usage and PyTorch integration
- [Benchmarking](benchmarking.md): Performance evaluation methodology